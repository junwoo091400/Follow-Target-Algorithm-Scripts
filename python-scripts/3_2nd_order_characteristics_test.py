# -*- coding: utf-8 -*-
"""3 : 2nd order characteristics test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p2iewPAhtGggCX6_Re_NfurSIWJoCCkW

# Install and Import
"""

!pip install pyulog
!pip install matplotlib

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
from pyulog import ULog
import pandas as pd

# tuple for hints
from typing import Tuple
import random

import matplotlib.pyplot as plt
import altair as alt

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

"""#Test Data processing"""

MESSAGES_LEN = 100
target_estimator_timestamps = np.ndarray((MESSAGES_LEN, 1))
target_estimator_pos = np.ndarray((MESSAGES_LEN, 3))
target_estimator_vel = np.ndarray((MESSAGES_LEN, 3))
target_estimator_accel = np.ndarray((MESSAGES_LEN, 3))

"""### Real Flight Data Generator"""

def get_concatenated_pos_vel_accel_data(msg_data_dict):
  # Get the data length for this log
  data_len = len(msg_data_dict['timestamp'])
  print('Data Length :', data_len)

  pos_0 = msg_data_dict['pos_est[0]'].reshape([data_len, 1])
  pos_1 = msg_data_dict['pos_est[1]'].reshape([data_len, 1])
  pos_2 = msg_data_dict['pos_est[2]'].reshape([data_len, 1])

  pos_concatenated = np.concatenate((pos_0, pos_1, pos_2), axis = 1)
  
  vel_0 = msg_data_dict['vel_est[0]'].reshape([data_len, 1])
  vel_1 = msg_data_dict['vel_est[1]'].reshape([data_len, 1])
  vel_2 = msg_data_dict['vel_est[2]'].reshape([data_len, 1])

  vel_concatenated = np.concatenate((vel_0, vel_1, vel_2), axis = 1)

  accel_0 = msg_data_dict['acc_est[0]'].reshape([data_len, 1])
  accel_1 = msg_data_dict['acc_est[1]'].reshape([data_len, 1])
  accel_2 = msg_data_dict['acc_est[2]'].reshape([data_len, 1])

  accel_concatenated = np.concatenate((accel_0, accel_1, accel_2), axis = 1)

  return (pos_concatenated, vel_concatenated, accel_concatenated)

'''
Generates the target estimator data out of an actual flight log (uLog)
'''
def generate_data_from_ulog(ulog_fname):
  ulog = ULog(ulog_fname)
  data_list = ulog.data_list
  # Go through the all the different messages and pick the one we want
  for data in data_list:
    msg_name = data.name
    if msg_name == 'follow_target_estimator':
        msg_data = data.data
        pos, vel, accel = get_concatenated_pos_vel_accel_data(msg_data)
        return (pos, vel, accel, msg_data['timestamp'])

"""### Fake Data Generator (100 Hz)

#### Circle
"""

def generate_target_poses_on_circle() -> Tuple[np.ndarray, np.ndarray, int, float]:
    """
    Generates target poses along a circle.
    Return Tuple( target positions, target velocities, time vector )
    """

    time_start = 0.  # [s]
    time_stop = 50.  # [s]
    time_step = 0.01  # [s], 100Hz trajectory setpoint generation

    circle_radius = 30.  # [m]
    accel_period_1 = [1.3, 7.]  # [s]
    accel_period_2 = [18., 25.]  # [s]

    # convert constant radius and tangential acceleration to reasonable angular acceleration
    linear_accel = 1.5  # [m/s^2]
    angular_accel = linear_accel / circle_radius  # [rad/s^2]

    time_vector = np.arange(time_start, time_stop, time_step)
    len_time = len(time_vector)

    # allocate and initialize
    angle = np.zeros(len_time)
    angular_rate = np.zeros(len_time)
    target_position = np.zeros([len_time, 3])
    target_velocity = np.zeros([len_time, 3])
    target_acceleration = np.zeros([len_time, 3])

    # simulate target motion
    for k in range(len_time):
        
        # system input calculation
        if time_vector[k] >= accel_period_1[0] and time_vector[k] < accel_period_1[1]:
            # first accelerate clockwise ...
            angular_accel_k = angular_accel  # [rad/s^2]

        elif time_vector[k] >=  accel_period_2[0] and time_vector[k] < accel_period_2[1]:
            # ... and then reverse after a while
            angular_accel_k = -angular_accel  # [rad/s^2]

        else:
            angular_accel_k = 0.  # [rad/s^2]

        # state output
        target_position[k, 0] = circle_radius * np.cos(angle[k])
        target_position[k, 1] = circle_radius * np.sin(angle[k])

        target_velocity[k, 0] = circle_radius * -np.sin(angle[k]) * angular_rate[k]
        target_velocity[k, 1] = circle_radius * np.cos(angle[k]) * angular_rate[k]

        target_acceleration[k, 0] = circle_radius * (angular_rate[k] ** 2) * -np.cos(angle[k])
        target_acceleration[k, 1] = circle_radius * (angular_rate[k] ** 2) * -np.sin(angle[k])

        # euler integration
        if k < len_time - 1:
            angle[k + 1] = angle[k] + angular_rate[k] * time_step
            angular_rate[k + 1] = angular_rate[k] + angular_accel_k * time_step

    return target_position, target_velocity, target_acceleration, (time_vector * 1E6) # Scale time vector up to microseconds unit

"""#### Jerky 2d motion"""

def generate_target_poses_jerky() -> Tuple[np.ndarray, np.ndarray, int, float]:
    """
    Return Tuple( target positions, target velocities, time vector )
    """

    time_start = 0.  # [s]
    time_stop = 30.  # [s]
    time_step = 0.01  # [s], 100Hz trajectory setpoint generation

    # Acceleration setting lists [(start time, end time), (acceleration_x, acceleration_y)]
    # (nan, nan) acceleration means vehicle should come to a halt
    # (inf, inf) means vehicle goes into a complete random motion
    accel_list = [[(1., 3.), (3.0, 0.0)],
                  [(5., 8.), (-4.0, 2.0)],
                  [(10., 13.), (4.0, -2.0)],
                  [(15., 17.), (np.nan, np.nan)],
                  [(17., 30.), (np.inf, np.inf)]
                  ]

    time_vector = np.arange(time_start, time_stop, time_step)
    len_time = len(time_vector)

    # allocate and initialize
    target_position = np.zeros([len_time, 3])
    target_velocity = np.zeros([len_time, 3])
    target_acceleration = np.zeros([len_time, 3])
    accel_vector = np.array([0., 0., 0.])

    # Parameters
    random_jerk_max = np.array([55.0, 55.0, 0.]) # Maximum random jerk values
    random_vel_max = np.array([5.0, 5.0, 0.])
    random_accel_max = np.array([10.0, 10.0, 0.])

    # simulate target motion
    for k in range(len_time):
        current_time = time_vector[k]

        accel_setting_completed = False
        for accel_idx in range(len(accel_list)):
          if current_time >= accel_list[accel_idx][0][0] and current_time < accel_list[accel_idx][0][1]:
            accel_setting = accel_list[accel_idx][1]

            if(np.isnan(accel_setting[0])):
              # NAN : Accel command to make velocity zero at the end time
              time_left_till_halt = (accel_list[accel_idx][0][1] - current_time)
              accel_vector = (-target_velocity[k]) / time_left_till_halt

            elif(accel_setting[0] == np.inf):
              # INF : Random acceleration
              accel_vector = np.multiply([random.uniform(-1., 1.), random.uniform(-1., 1.), 0.], random_accel_max)

            else:
              # Normal accelration setting
              accel_vector = np.array([accel_setting[0], accel_setting[1], 0.])
            accel_setting_completed = True
          
        # If there's no acc setting, set acceleration to zero.
        if(not accel_setting_completed):
          accel_vector = np.array([0., 0., 0.])

        # euler integration
        if k < len_time - 1:
            target_position[k + 1] = target_position[k] + target_velocity[k] * time_step
            target_velocity[k + 1] = target_velocity[k] + accel_vector * time_step
            target_acceleration[k + 1] = accel_vector

    return target_position, target_velocity, target_acceleration, (time_vector * 1E6) # Scale time vector up to microseconds unit

"""### Generate Target pose for filter testing"""

# Cricle data
#target_estimator_pos, target_estimator_vel, target_estimator_accel, target_estimator_timestamps = generate_target_poses_on_circle()

# Jerky data
#target_estimator_pos, target_estimator_vel, target_estimator_accel, target_estimator_timestamps = generate_target_poses_jerky()

# Real data
ulog_fname = '/content/drive/MyDrive/PX4 Development/Followme/FollowTarget_FullRate_2022-03-03_10-55-16.ulg'
target_estimator_pos, target_estimator_vel, target_estimator_accel, target_estimator_timestamps = generate_data_from_ulog(ulog_fname)

"""### Visualize Target position trajectory briefly"""

fig = go.Figure()

# Plot simple 2D trajectory
fig.add_trace(go.Scatter(x=target_estimator_pos[:,0], y=target_estimator_pos[:,1]))
fig.update_layout(title_text="Target position setpoint visualization on 2D plane")

fig.show()

"""# Second Order Filter Implementation"""

class SecondOrderFilter:

    def __init__(self,
            natural_frequency: float,  # [rad/s]
            damping_ratio: float
        ):
        
        self._spring_constant = natural_frequency ** 2
        self._damping_coefficient = 2 * damping_ratio * natural_frequency

        self._state = np.nan
        self._rate = np.nan
        self._accel = np.nan

    
    def get_state(self) -> np.ndarray:
        return self._state


    def get_rate(self) -> np.ndarray:
        return self._rate


    def get_accel(self) -> np.ndarray:
        return self._accel

    # Check if any state / rate / accel has NaN value in it
    def has_nan(self) -> bool:
      sum = np.sum(self._state)
      sum += np.sum(self._rate)
      sum += np.sum(self._accel)

      return np.isnan(sum)

    def print_states(self):
      print('Filter state :', self._state, ', rate :', self._rate, ', accel :', self._accel)

    def _integrate(self,
            state: np.ndarray,
            rate: np.ndarray,
            dt: float
        ) -> np.ndarray:
        """
        Euler integration
        """

        return state + rate * dt

    
    def update(self,
            state_setpoint: np.ndarray,
            rate_setpoint: np.ndarray,
            dt: float  # XXX: need to put checks on dt
        ):

        # signal errors
        state_error = state_setpoint - self._state
        rate_error = rate_setpoint - self._rate

        # double integration
        self._accel = state_error * self._spring_constant + rate_error * self._damping_coefficient
        self._rate = self._integrate(self._rate, self._accel, dt)
        self._state = self._integrate(self._state, self._rate, dt)

    
    def reset(self,
            state: np.ndarray,
            rate: np.ndarray,
            accel: np.ndarray
        ):

        self._state = state
        self._rate = rate
        self._accel = accel

"""# Simulation"""

# Parameteres to control filter behavior
FILTER_RESET_TIMEOUT_SECONDS = 1.5 # If the latest update was more than this time in the 'past', reset the filter to avoid divergence.

# Simulates a 2nd order filter with a given natural frequency & damping ratio
def simulate_filter(natural_freq: float, damping_ratio: float, timestamps: np.ndarray, 
                    position_estimates: np.ndarray, velocity_estimates: np.ndarray, 
                    acceleration_estimates: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
  global FILTER_RESET_TIMEOUT_SECONDS
  
  print(natural_freq)

  position_filtered = np.array([position_estimates[0]])
  velocity_filtered = np.array([velocity_estimates[0]])
  acceleration_filtered = np.array([acceleration_estimates[0]])
  previous_timestamp = timestamps[0]

  filter = SecondOrderFilter(natural_freq, damping_ratio)

  # Reset the filter to the first data
  filter.reset(position_filtered[-1], velocity_filtered[-1], acceleration_filtered[-1])
  print('Filter initialized for simulation!')
  filter.print_states()

  #first_timestamp = timestamps[0] # For start time reference
  
  for i in range(1, len(timestamps)):
    timestamp = timestamps[i]
    deltaT = (timestamp - previous_timestamp) / 1E6 # Microseconds to Seconds
    previous_timestamp = timestamp

    if filter.has_nan() or deltaT > FILTER_RESET_TIMEOUT_SECONDS: # If states are NaN or incoming data has timed out
      print('Filter resetting... iteration :', i, ', deltaT :', deltaT, ', time :', (timestamp) / 1E6)
      filter.print_states()
      filter.reset(position_estimates[i], velocity_estimates[i], acceleration_estimates[i])
      filter.print_states()
    else:
      # Update the filter
      # try:
      filter.update(position_estimates[i], velocity_estimates[i], deltaT)
      # finally:
      #   print('Filter Update error!')
      #   print(filter.get_state())

    position_filtered = np.append(position_filtered, np.reshape(filter.get_state(), (1,3)), axis=0)
    velocity_filtered = np.append(velocity_filtered, np.reshape(filter.get_rate(), (1,3)), axis=0)
    acceleration_filtered = np.append(acceleration_filtered, np.reshape(filter.get_accel(), (1,3)), axis=0)
  
  return (position_filtered, velocity_filtered, acceleration_filtered)

# Frequencies we are going to test
natural_frequency_list = [0.1, 0.5, 1.0, 2.0, 4.0, 8.0, 14.0] # [rad/s]
damping_ratio = 0.7071

simulated_posvelaccels = []

for freq in natural_frequency_list:
  pos, vel, accel = simulate_filter(freq, damping_ratio, target_estimator_timestamps, 
                                    target_estimator_pos, target_estimator_vel, 
                                    target_estimator_accel)
  combined_results = np.concatenate((pos,vel,accel), axis=1)
  simulated_posvelaccels.append(combined_results)

"""# Visualization

## [IMPORTANT!] Simulated Data

### Position Data
"""

timestamps = target_estimator_timestamps / 1E6 # Converts microseconds into seconds
# fig = make_subplots(rows=3, cols=1)

for pos_idx in range(3):
  fig = go.Figure()
  # Add RAW Target estimator data
  fig.add_trace(go.Scatter(x=timestamps, y=target_estimator_pos[:,pos_idx],
                      mode='lines',
                      name='Target Estimator Pos_{}'.format(pos_idx)))

  # Add Filtered Data
  for i in range(len(natural_frequency_list)):
    freq = natural_frequency_list[i]
    # Only show up until 2 rad/s natural freq. Since upwards, the data is really dirty / diverging
    # if freq>2:
    #   continue
    posvelaccel = simulated_posvelaccels[i]
    fig.add_trace(go.Scatter(x=timestamps, y=posvelaccel[:,pos_idx],
                      mode='lines',
                      name=str(freq)+'[Rad/s] Pos_{}'.format(pos_idx)))

  # In High natural Frequency filtered outputs, value goes super high, so clip the view
  fig.update_yaxes(range=[-50, 50])
  fig.update_layout(title_text='Position outputs from estimator & filters')

  fig.show()

"""### Velocity Data"""

timestamps = target_estimator_timestamps / 1E6 # Converts microseconds into seconds

for vel_idx in range(3):
  fig = go.Figure()
  # Add RAW Target estimator data
  fig.add_trace(go.Scatter(x=timestamps, y=target_estimator_vel[:,vel_idx],
                      mode='lines',
                      name='Target Estimator Vel_{}'.format(vel_idx)))

  # Add Filtered Data
  for i in range(len(natural_frequency_list)):
    freq = natural_frequency_list[i]
    # Only show up until 2 rad/s natural freq. Since upwards, the data is really dirty / diverging
    # if freq>2:
    #   continue
    posvelaccel = simulated_posvelaccels[i]
    fig.add_trace(go.Scatter(x=timestamps, y=posvelaccel[:,3 + vel_idx],
                      mode='lines',
                      name=str(freq)+'[Rad/s] Vel_{}'.format(vel_idx)))

  # In High natural Frequency filtered outputs, value goes super high, so clip the view
  fig.update_yaxes(range=[-10, 10])
  fig.update_layout(title_text='Velocity outputs from estimator & filters')

  fig.show()

"""### Acceleration Data"""

timestamps = target_estimator_timestamps / 1E6 # Converts microseconds into seconds

for accel_idx in range(3):
  fig = go.Figure()
  # Add RAW Target estimator data
  fig.add_trace(go.Scatter(x=timestamps, y=target_estimator_accel[:,accel_idx],
                      mode='lines',
                      name='Target Estimator Accel_{}'.format(accel_idx)))

  # Add Filtered Data
  for i in range(len(natural_frequency_list)):
    freq = natural_frequency_list[i]
    # Only show up until 2 rad/s natural freq. Since upwards, the data is really dirty / diverging
    # if freq>2:
    #   continue
    posvelaccel = simulated_posvelaccels[i]
    fig.add_trace(go.Scatter(x=timestamps, y=posvelaccel[:,6 + accel_idx],
                      mode='lines',
                      name=str(freq)+'[Rad/s] Accel_{}'.format(accel_idx)))

  # In High natural Frequency filtered outputs, value goes super high, so clip the view
  fig.update_yaxes(range=[-2, 2])
  fig.update_layout(title_text='Acceleration outputs from estimator & filters')

  fig.show()

"""## Raw Data

"""

x = target_estimator_timestamps / 1E6 # Converts microseconds into seconds
print(x)

pos_0 = target_estimator_pos[:, 0]
vel_0 = target_estimator_vel[:, 0]

plt.plot(x, pos_0, 'r.-', label = 'target_estimator_pos0')
plt.plot(x, vel_0, 'b.-', label = 'target_estimator_vel0')

plt.show()

timestamps = target_estimator_timestamps / 1E6 # Converts microseconds into seconds

pos_0 = target_estimator_pos[:, 1]
vel_0 = target_estimator_vel[:, 1]

fig = go.Figure()
fig.add_trace(go.Scatter(x=timestamps, y=pos_0,
                    mode='lines+markers',
                    name='Pos_0'))
fig.add_trace(go.Scatter(x=timestamps, y=vel_0,
                    mode='lines+markers',
                    name='Vel_0'))

fig.show()

# pd_data = pd.DataFrame({
#     'x' : x,
#     'pos_0' : pos_0,
#     'vel_0' : vel_0
# })

# alt.Chart(pd_data).mark_line().encode(x='x',y='pos_0').properties(width=1500,height=1000)

"""## [IMPORTANT!] Standard Deviation Calculation"""

def calculate_stddev(data_1, data_2):
  valid_data_count = 0
  sq_sum = 0.0

  for i in range(len(data_1)):
    a1 = data_1[i]
    a2 = data_2[i]
    if(np.isnan(a1) or np.isinf(a1) or np.isnan(a2) or np.isinf(a2)):
      continue
    else:
      valid_data_count += 1
      sq_sum += (a1 - a2) ** 2

  return ((sq_sum ** 0.5) / valid_data_count, valid_data_count)

# Each Frequency, having STDDEV for pos, vel, accel.
stddev_by_freq = np.ndarray((len(natural_frequency_list),9))

# Pos
for i in range(len(natural_frequency_list)):
  freq = natural_frequency_list[i]
  posvelaccel = simulated_posvelaccels[i]
  for j in range(3):
    ground_truth = target_estimator_pos[:,j]
    filtered_data = posvelaccel[:, j]
    stddev, valid_count = calculate_stddev(ground_truth, filtered_data)
    print('Freq :', freq, '[rad/s], ', 'Pos :', j, ' -> STDEV :', stddev, ', Valid Datapoints :', valid_count)
    stddev_by_freq[i,j] = stddev
# Vel
for i in range(len(natural_frequency_list)):
  freq = natural_frequency_list[i]
  posvelaccel = simulated_posvelaccels[i]
  for j in range(3, 6):
    ground_truth = target_estimator_vel[:,j%3]
    filtered_data = posvelaccel[:, 3 + j]
    stddev, valid_count = calculate_stddev(ground_truth, filtered_data)
    print('Freq :', freq, '[rad/s], ', 'Vel :', j, ' -> STDEV :', stddev, ', Valid Datapoints :', valid_count)
    stddev_by_freq[i,j] = stddev

# Accel
for i in range(len(natural_frequency_list)):
  freq = natural_frequency_list[i]
  posvelaccel = simulated_posvelaccels[i]
  for j in range(6, 9):
    ground_truth = target_estimator_accel[:,j%3]
    filtered_data = posvelaccel[:, j]
    stddev, valid_count = calculate_stddev(ground_truth, filtered_data)
    print('Freq :', freq, '[rad/s], ', 'Accel :', j, ' -> STDEV :', stddev, ', Valid Datapoints :', valid_count)
    stddev_by_freq[i,j] = stddev

# Draw Figure
# x = [str(freq) for freq in natural_frequency_list]
x_labels = ['Pos_0', 'Pos_1', 'Pos_2', 'Vel_0', 'Vel_1', 'Vel_2', 'Accel_0', 'Accel_1', 'Accel_2']

fig = go.Figure()

for i in range(len(natural_frequency_list)):
  freq = natural_frequency_list[i]
  fig.add_trace(go.Bar(x=x_labels, y=stddev_by_freq[i], name = str(freq) + '[rad/s]'))

fig.update_layout(barmode='group', title_text='Standard Deviation for each pose components')
fig.update_yaxes(range=[0,1])

fig.show()